{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2399214",
   "metadata": {},
   "source": [
    "# Data Acquisition and Processing Systems (DaPS) (ELEC0136)\n",
    "\n",
    "__Final Assignment__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606897d4",
   "metadata": {},
   "source": [
    "## Package Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334fdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# APIs\n",
    "import yfinance as yf\n",
    "import alpha_vantage\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "# Data storage and processing\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "# Data exploration and visualisation\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import friedmanchisquare\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import kaleido\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# Modelling\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8dd23f",
   "metadata": {},
   "source": [
    "## Data Acquisition:\n",
    "\n",
    "We group data sources into classes with which we can:\n",
    "\n",
    "1. `load` -- request data from an API or parse local plaintext storage files, bringing the data into the scope\n",
    "2. `write` -- store the ingested data using media which proide engineering benefits to the performance of the system\n",
    "3. `fetch` -- not exactly a query system, but rather a much more perfomant data i/o routine from local 'hot' storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98003c6d",
   "metadata": {},
   "source": [
    "### `YahooFinance`\n",
    "\n",
    "This class sources, stores and fetches data from the Yahoo! finance plaform, either by an API request (using `yfinance`) or by parsing a local csv file sourced manually.\n",
    "\n",
    "#### Data:\n",
    "\n",
    "`YahooFinance.data` is where the ingested data is stored.\n",
    "It takes the form a `pandasDataFrame` with a `datetime` index.\n",
    "\n",
    "\n",
    "From this data structure, the data we have **and are interested in** is the following:\n",
    "\n",
    "* `Close` -- the closing price of MSFT stock\n",
    "* `Volume` -- the number of MSFT shares traded on any given day\n",
    "\n",
    "There may be other financial metrics of interest, in which case we might shop around for other APIs (such as AlphaVantage) to obtain metrics such as moving averages, long/short interest and other technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58c0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooFinance:\n",
    "    \"\"\"Closing price, opening, trading volumes, daily averages and other pricing value for a given ticker.\n",
    "    \n",
    "    **Using API**\n",
    "    \n",
    "    (DEFAULT)\n",
    "    \n",
    "    Set `_local=False` in constructor to indicate we are sourcing the data from the `yfinance` API.\n",
    "    You can modify the symbols (stock tickers) which can be fetched with the API call.\n",
    "    \n",
    "    **Using local file**\n",
    "    \n",
    "    Set `_local=True.\n",
    "    \n",
    "    Only market data for $MSFT is downloaded onto a local file at `data/raw/yfinance_MSFT.csv`.\n",
    "    This data will still be loaded and written to a feather file format.\n",
    "    The feather file format has more efficient read/write in terms of memory usage, but also speed.\n",
    "    \"\"\"\n",
    "    \n",
    "    RAW_PATH = \"data/raw/yfinance.csv\"\n",
    "    SAVE_PATH = \"data/store/market_data\"\n",
    "    \n",
    "    def __init__(self, _local=False, _savelocal=True, _symbols=\"MSFT\"):\n",
    "        self.local = _local\n",
    "        self.savelocal = _savelocal\n",
    "        self.symbols = _symbols\n",
    "        self.data = 0\n",
    "    \n",
    "    def load(self):\n",
    "        if self.savelocal == False:\n",
    "            print(\"Permission to load files from cold storage denied, instead try using `fetch`.\")\n",
    "            return 1\n",
    "        \n",
    "        if self.local == True:\n",
    "            print(\"Files loaded from: \" + self.RAW_PATH)\n",
    "            data = pd.read_csv(\"data/raw/yfinance_MSFT.csv\", index_col=0)\n",
    "            data['Volume'] = data['Volume'].values.astype(float)\n",
    "            data['Date'] = data.index.values\n",
    "            self.data = data\n",
    "        else:\n",
    "            print(\"Data requested from Yahoo! Finance API (via yfinance)\")\n",
    "            data = yf.download(self.symbols, start=\"2017-04-01\", end=\"2021-05-01\")\n",
    "            data['Volume'] = data['Volume'].values.astype(float)\n",
    "            data['Date'] = data.index.values\n",
    "            self.data = data\n",
    "        \n",
    "    def write(self):\n",
    "        if self.savelocal == False:\n",
    "            print(\"You do not have the permission to local storage, there is already a file written.\")\n",
    "        else:\n",
    "            if os.path.exists(self.SAVE_PATH):\n",
    "                os.remove(self.SAVE_PATH)\n",
    "            feather.write_feather(self.data, self.SAVE_PATH)\n",
    "            print(\"File written at: \" + self.SAVE_PATH)\n",
    "        \n",
    "    def fetch(self):\n",
    "        \"writing to feather unfortunately gets rid of datetime index, workaround needed in write()\"\n",
    "        if not os.path.exists(self.SAVE_PATH):\n",
    "            print(\"No saved data exists, ensure you have first loaded and saved external data locally.\")\n",
    "        else:\n",
    "            print(\"Data fetched from: \" + self.SAVE_PATH)\n",
    "            data = feather.read_feather(self.SAVE_PATH)\n",
    "            data.index = data['Date'].values; data = data.drop(['Date'], axis=1)\n",
    "            self.data = data\n",
    "            return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a018f",
   "metadata": {},
   "source": [
    "### `GoogleTrends`\n",
    "\n",
    "The class provides methods to source and manipulate data from the Google trends analysis tool.\n",
    "\n",
    "> This was initially attempted by using the `pytrends` package, a third-party wrapper for the Google trends API, however the data packets received did not match up to the raw data manually sourced from the [Google trends web dashboard](https://trends.google.co.uk/trends/?geo=GB).\n",
    "\n",
    "Google scores web trends on a scale from 0-100, zero being the least searched/interacted with and 100 garnering the maximum web activity from Google users.\n",
    "\n",
    "Three trends related to Microsoft Corporation were sourced based on the `Microsoft Corporation` tag:\n",
    "\n",
    "1. `Search` -- general search trends (somewhat based on volume of Microsoft related searches)\n",
    "2. `News` -- news headline and article metric (analogous to frequency of Microsoft being mentioned in news publications)\n",
    "3. `Financial` -- financial searches related to Microsoft (this trend seems intersting since one naively assumes that greater interest in Microsoft related finances should correlate to stock price movement)\n",
    "\n",
    "#### Data:\n",
    "\n",
    "`GoogleTrends.data` is the field that stores data whilst in scope, otherwise `fetch` can be used to perform a fast i/o read from feather local store.\n",
    "\n",
    "In betweent the sourcing and loading-into-scope stage of the data ingress, we bundle the separate trend dataframes into a single structure which can be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b2f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleTrends:\n",
    "    \"\"\"Trend data from Google search engine: general search, financial searches and mews for given company name.\n",
    "    \n",
    "    If you change the location of the manually sourced data (mostly csv files in 'data/raw'), you must remember\n",
    "    to appropriately modify the `RAW_PATH` field of the `GoogleTrends` class.\n",
    "    \"\"\"\n",
    "    \n",
    "    RAW_PATH = \"data/raw/\"\n",
    "    SAVE_PATH = \"data/store/google_trends\" # feather file database path\n",
    "    \n",
    "    def __init__(self, _local=True, _savelocal=True):\n",
    "        \"Used to download, store and load Google trend data from API or local sources.\"\n",
    "        self.local = _local\n",
    "        self.savelocal = _savelocal\n",
    "        self.data = 0\n",
    "                \n",
    "    def load(self):\n",
    "        if self.savelocal == False:\n",
    "            print(\"Permission to load files from cold storage denied, instead try using `fetch`.\")\n",
    "            return 1\n",
    "        \n",
    "        print(\"Files loaded from: \" + self.RAW_PATH)\n",
    "        def trend_import(file :str):\n",
    "            return pd.read_csv(self.RAW_PATH + file, index_col=0)[1:]\n",
    "\n",
    "        # bundle the three Google trend dataframes into a single structure\n",
    "        bundle = trend_import(\"gtrends_search.csv\")\n",
    "        bundle['Search'] = trend_import(\"gtrends_search.csv\").values.astype(float)\n",
    "        bundle['News'] = trend_import(\"gtrends_news.csv\").values.astype(float)\n",
    "        bundle['Financial'] = trend_import(\"gtrends_financial.csv\").values.astype(float)\n",
    "        bundle['Date'] = bundle.index.values\n",
    "        bundle.index.rename('Date', inplace=True)\n",
    "        bundle.index = pd.to_datetime(bundle.index)\n",
    "        del bundle[bundle.columns[0]]\n",
    "        \n",
    "        self.data = bundle\n",
    "    \n",
    "    def write(self):\n",
    "        if self.savelocal == False:\n",
    "            print(\"Local disk storage is disabled in the `DataConfig` config object!\")\n",
    "        else:\n",
    "            if os.path.exists(self.SAVE_PATH):\n",
    "                os.remove(self.SAVE_PATH)\n",
    "            feather.write_feather(self.data, self.SAVE_PATH)\n",
    "            print(\"Data written at: \" + self.SAVE_PATH)\n",
    "            \n",
    "    def fetch(self):\n",
    "        \"Google trends data, fetch from fast I/O file storage (using feather from Apache Arrow)\"\n",
    "        if not os.path.exists(self.SAVE_PATH):\n",
    "            print(\"No saved data exists, ensure you have first loaded and saved external data locally.\")\n",
    "        else:\n",
    "            print(\"Data fetched from: \" + self.SAVE_PATH)\n",
    "            data = feather.read_feather(self.SAVE_PATH)\n",
    "            data.index = data['Date'].values; data = data.drop(['Date'], axis=1)\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            self.data = data\n",
    "            return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ccb7d",
   "metadata": {},
   "source": [
    "## Data Processing:\n",
    "\n",
    "General wrangling, cleaning, preprocessing, feature generation and overall preparation for the exploration/visualisation.\n",
    "\n",
    "Things to do:\n",
    "\n",
    "1. fill in the gaps (weekends and sampling periods) for both google trends and stock pricing data\n",
    "    - generate day-date index from 01/04/2017 to 01/05/2021 and fill Nan values with previous non-NaN data-point\n",
    "2. split datasets into train / validate groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6918186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger(yahoo, google):\n",
    "    \"Consistent datetime index avoiding NaN values issues for GoogleTrends & YahooFinance data structures.\"\n",
    "    \n",
    "    def linear_df(dff):\n",
    "        df = pd.DataFrame(\n",
    "            index=pd.date_range(\n",
    "                start=str(dff.index.values[0])[0:10],\n",
    "                end=str(dff.index.values[-1])[0:10]))\n",
    "        return df\n",
    "    \n",
    "    df = linear_df(yahoo)\n",
    "    df = df.join(google)\n",
    "    df = df.interpolate(method='time')\n",
    "    \n",
    "    df = df.join(yahoo[['Close', 'Volume']])\n",
    "    df = df.dropna(subset=['Close'])\n",
    "    \n",
    "    df['Search'][0] = df['Search'][1]\n",
    "    df['News'][0] = df['News'][1]\n",
    "    df['Financial'][0] = df['Financial'][1]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0cd91",
   "metadata": {},
   "source": [
    "## Data Exploration:\n",
    "\n",
    "### Static Analysis and Hypothesis Testing\n",
    "\n",
    "The following hypothesis tests should allow us to gain insight into the statistical relationship between features and stock price.\n",
    "\n",
    "#### 1. ANOVA Test\n",
    "\n",
    "We would like to find out if the classes in question are the same or different distributions.\n",
    "\n",
    "##### Assumptions:\n",
    "\n",
    "* Sample observations are independent and identically distributed\n",
    "* Sample observations are normally distributed\n",
    "* Sample observations have the same variance\n",
    "\n",
    "##### Hypothesis:\n",
    "\n",
    "* __H0__ : When all samples' mean values are the same\n",
    "* __H1__ : When one ore more sample are very different statistically.\n",
    "\n",
    "##### Outcome:\n",
    "\n",
    "The pvalue <<< 0.05, therefore the means of the samples for:\n",
    "\n",
    "* MSFT closing price\n",
    "* MSFT Google trends search score\n",
    "* MSFT Google trends financial search score\n",
    "\n",
    "__are not equal__.\n",
    "\n",
    "\n",
    "#### 2. Friedman Test\n",
    "\n",
    "We will perform a statistical test to determine whether the two or more paired samples: MSFT closing price, MSFT Google trends search score and MSFT Google trends financial score are equal or not by considering their distribution.\n",
    "\n",
    "##### Assumptions:\n",
    "\n",
    "* Observations samples are independent and identically distributed\n",
    "* Observations samples can be ranked\n",
    "* Observations across samples are paired\n",
    "\n",
    "##### Hypothesis:\n",
    "\n",
    "* __H0__ : The distributions of all samples are equal\n",
    "* __H1__ : The distributions of one or more samples are not equal\n",
    "\n",
    "##### Outcome:\n",
    "\n",
    "The pvalue <<< 0.05, therefore the distributions of the samples for:\n",
    "\n",
    "* MSFT closing price\n",
    "* MSFT Google trends search score\n",
    "* MSFT Google trends financial search score\n",
    "\n",
    "__are not equal__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ab7cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticAnalysis:\n",
    "    \"Statistical hypothesis tests.\"\n",
    "    \n",
    "    def __init__(self, _data):\n",
    "        self.data = _data\n",
    "        self.anova = None\n",
    "        self.friedman = None\n",
    "        self.dickey_fuller = None\n",
    "        \n",
    "    def friedman_test(self):\n",
    "        s, p = friedmanchisquare(self.data['Close'],\n",
    "                                 self.data['Search'],\n",
    "                                 self.data['Financial'])\n",
    "        self.friedman = p\n",
    "        print('stat=%.3f, p=%.3f' % (s, p))\n",
    "        if p > 0.05:\n",
    "            print('H0: Probably the same distribution')\n",
    "        else:\n",
    "            print('H1: Probably different distributions')\n",
    "            \n",
    "    def anova_test(self):\n",
    "        s, p = f_oneway(self.data['Close'],\n",
    "                        self.data['Search'],\n",
    "                        self.data['Financial'])\n",
    "        self.anova = p\n",
    "        print('stat=%.3f, p=%.3f' % (s, p))\n",
    "        if p > 0.05:\n",
    "            print('H0: Probably the same distribution')\n",
    "        else:\n",
    "            print('H1: Probably different distributions')\n",
    "            \n",
    "    def dickey_fuller_test(self):\n",
    "        test = adfuller(self.data['Close'])\n",
    "        p = test[2]\n",
    "        s = test[1]\n",
    "        self.dickey_fuller = p\n",
    "        print('stat=%.3f, p=%.3f' % (s, p))\n",
    "        if p > 0.05:\n",
    "            print('H0: Time series is non-stationary, has time-dependent structure.')\n",
    "        else:\n",
    "            print('H1: Time series is stationary, no time-dependent structure.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928674cb",
   "metadata": {},
   "source": [
    "### Time Series Analysis\n",
    "\n",
    "We will decompose our timeseries data using statistical methods.\n",
    "\n",
    "These should help use get a better understanding of what type of signals drive the behaviour of the closing stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81910434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatic_tsd(df):\n",
    "    \"Automatic time series decompostion.\"\n",
    "    close_result = seasonal_decompose(df['Close'], model='additive',period=50,extrapolate_trend='freq')\n",
    "    search_result = seasonal_decompose(df['Search'], model='additive',period=20,extrapolate_trend='freq')\n",
    "    news_result = seasonal_decompose(df['News'], model='additive',period=20,extrapolate_trend='freq')\n",
    "    financial_result = seasonal_decompose(df['Financial'], model='additive',period=20,extrapolate_trend='freq')\n",
    "    \n",
    "    def tsd_plot(df, result, savefile):\n",
    "        fig = make_subplots(rows=5, cols=1,\n",
    "                            subplot_titles=(\"Observed\",\"Trend\",\"Seasonality\",\"Residual\"))\n",
    "        fig. add_trace(go.Scatter(x=df.index, y=result.observed,name=\"Observed\"),\n",
    "                                  row=1,col=1)\n",
    "        fig. add_trace(go.Scatter(x=df.index, y=result.trend, name=\"Trend\"),\n",
    "                                  row=2,col=1)\n",
    "        fig. add_trace(go.Scatter(x=df.index, y=result.seasonal, name=\"Seasonality\"),\n",
    "                                  row=3,col=1)\n",
    "        fig. add_trace(go.Scatter(x=df.index, y=result.resid, name=\"Residual\"),\n",
    "                                  row=4,col=1)\n",
    "        fig.update_layout(autosize=False,\n",
    "                          width=800,\n",
    "                          height=800,)\n",
    "        print(\"Figure saved to file at: \" + savefile)\n",
    "        fig.write_image(savefile)\n",
    "    \n",
    "    tsd_plot(df, close_result, \"images/tsd_price.png\")\n",
    "    tsd_plot(df, search_result, \"images/tsd_search.png\")\n",
    "    tsd_plot(df, news_result, \"images/tsd_news.png\")\n",
    "    tsd_plot(df, financial_result, \"images/tsd_financial.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c021c9f",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "#### Google Trends Signal Smoothing\n",
    "\n",
    "In order to smoothly merge the Google trends date-indexed score with the financial data from Yahoo! Finance, we needed to convert a discrete and sparse timeseries dataset into a daily-timeseries dataset, which we would then be able to merge with the Yahoo! dataframe.\n",
    "\n",
    "In the end, we have used a floodfill-like techniques in which we chronologically extend known trend score over dates previously with NaN values, until then next sample value is reached and becomes the next filler value.\n",
    "This results in a timeseries trend signal which displays *plateaus* where the known values were 'flooded'.\n",
    "\n",
    "The function `plot_trend_filling` showcases an example of this floodfilling effect, compared with the original sparse signal.\n",
    "\n",
    "A copy of the figure produced is saved at: `images/google_trends_filling_effect.png` for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b057177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend_filling(df_merged, df_gtrends):\n",
    "    \"Plot of the effect of 'filling-in' trend data to fit linearly over datetime index range.\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df_merged.index, y=df_merged['Search'],\n",
    "                             mode='lines',\n",
    "                             name='Filled'))\n",
    "    fig.add_trace(go.Scatter(x=df_gtrends.index, y=df_gtrends['Search'],\n",
    "                             mode='lines',\n",
    "                             name='Original'))\n",
    "    fig.update_layout(title=\"Google Search Trend Data for 'Microsoft Corporation' - Discrete vs. 'Filled'\",\n",
    "                      xaxis_title='Date',\n",
    "                      yaxis_title='Score')\n",
    "    fig.write_image(\"images/google_trends_filling_effect.png\")\n",
    "    print(\"Figure saved to file at: images/google_trends_filling_effect.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511f666",
   "metadata": {},
   "source": [
    "#### Feature Correlation Heatmap\n",
    "\n",
    "No machine learning or data science task would be complete without one.\n",
    "The quintessential visualization technique serves to aid with feature selection, if there is a good correlation between an input feature and the target outputs it is more likely that meaningful patterns can be learned from the feature.\n",
    "\n",
    "`plot_correlation_heatmap` generates the heatmap figure, whilst `correlation_matrix` calculates the 2D-array of correlation values.\n",
    "\n",
    "The heatmap figure can be found at: `images/closing_price_feature_correlation.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee652f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlation_analysis(df):\n",
    "    close = np.array(df['Close'].values)\n",
    "    search = np.array(df['Search'].values)\n",
    "    news = np.array(df['News'].values)\n",
    "    financial = np.array(df['Financial'].values)\n",
    "    \n",
    "    cc_search = sm.tsa.stattools.ccf(search, close, adjusted=False)\n",
    "    cc_news = sm.tsa.stattools.ccf(news, close, adjusted=False)\n",
    "    cc_financial = sm.tsa.stattools.ccf(financial, close, adjusted=False)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=cc_search,\n",
    "                             mode='lines',\n",
    "                             name='Search-Close'))\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=cc_news,\n",
    "                             mode='lines',\n",
    "                             name='News-Close'))\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=cc_financial,\n",
    "                             mode='lines',\n",
    "                             name='Financial-Close'))\n",
    "    fig.update_layout(title=\"Closing Price & Features Cross-Correlation Pairs\",\n",
    "                      xaxis_title='Date',\n",
    "                      yaxis_title='Cross Correlation')\n",
    "    fig.write_image(\"images/ts_cross_correlation.png\")\n",
    "    print(\"Figure saved to file at: images/ts_cross_correlation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006ff79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    \"Extremely hacky and non-composable way to making a feature correlation matrix.\"\n",
    "    def corr(df, key):\n",
    "        return [df[key].corr(df['Close']),\n",
    "                df[key].corr(df['Volume']),\n",
    "                df[key].corr(df['Search']),\n",
    "                df[key].corr(df['News']),\n",
    "                df[key].corr(df['Financial'])]\n",
    "    \n",
    "    close_corr = corr(df, 'Close')\n",
    "    volume_corr = corr(df, 'Volume')\n",
    "    search_corr = corr(df, 'Search')\n",
    "    news_corr = corr(df, 'News')\n",
    "    financial_corr = corr(df, 'Financial')\n",
    "    \n",
    "    return [close_corr, volume_corr, search_corr, news_corr, financial_corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6d38243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df):\n",
    "    \"Timeseries feature correlations to MSFT closing price.\"\n",
    "    features = ['Close', 'Volume', 'Search', 'News', 'Financial']\n",
    "    matrix = correlation_matrix(df)\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(z=matrix,\n",
    "                                    x=features,\n",
    "                                    y=features,\n",
    "                                    colorscale='Viridis'))\n",
    "    fig.update_layout(title='MSFT Stock Price Timeseries Feature Correlation Map')\n",
    "    fig.write_image(\"images/closing_price_feature_correlation.png\")\n",
    "    print(\"Figure saved to file at: images/closing_price_feature_correlation.png\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0247dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(df):\n",
    "    \"A visual look at the moving average (MA) behaviour of MSFT\"\n",
    "    short_rolling = df.rolling(window=20).mean()\n",
    "    long_rolling = df.rolling(window=80).mean()\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['Close'],\n",
    "                         mode='lines',\n",
    "                         name='Close'))\n",
    "    fig.add_trace(go.Scatter(x=df.index[19:], y=short_rolling['Close'],\n",
    "                         mode='lines',\n",
    "                         name='Short MA'))\n",
    "    fig.add_trace(go.Scatter(x=df.index[79:], y=long_rolling['Close'],\n",
    "                         mode='lines',\n",
    "                         name='Long MA'))\n",
    "    fig.update_layout(title='MSFT Closing Price with Long and Short Moving Average',\n",
    "                 xaxis_title='Date',\n",
    "                 yaxis_title='Price ($)')\n",
    "    \n",
    "    fig.write_image(\"images/moving_average.png\")\n",
    "    print(\"Figure saved to file at: images/moving_average.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38334801",
   "metadata": {},
   "source": [
    "### Data Preparation for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "135f0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scale_data(data, only_price=True):\n",
    "    \"Scale train data, then test data independently, to avoid leaking information forward.\"\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    \n",
    "    train = data[:-21]\n",
    "    test = data[-21:]\n",
    "    \n",
    "    if only_price == True:\n",
    "        train = scaler.fit_transform(train.reshape(-1, 1))\n",
    "        test = scaler.transform(test.reshape(-1, 1))\n",
    "    else:\n",
    "        train = scaler.fit_transform(train.reshape(-1, 4))\n",
    "        test = scaler.transform(test.reshape(-1, 4))\n",
    "    \n",
    "    return (np.concatenate((train, test)), scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe2c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df, window, only_price=True):\n",
    "    \"Pre-model data preparation, behaviour defined by whether using `only_price` as feature or multivariate.\"\n",
    "    if only_price == True:\n",
    "        data, scaler = split_scale_data(np.array(df['Close'], dtype='float'),\n",
    "                                        only_price)\n",
    "    else:\n",
    "        # scale each feature independently, only keep scaler values for price data for la\n",
    "        data, scaler = split_scale_data(np.array(df[['Close','Search','News','Financial']], dtype='float'),\n",
    "                                        only_price)\n",
    "\n",
    "    windowed_data = []\n",
    "    \n",
    "    for i in range((len(data) - window)):\n",
    "        windowed_data.append(data[i:i+window])\n",
    "    \n",
    "    windowed_data = np.array(windowed_data, dtype='float')\n",
    "    \n",
    "    x_train = windowed_data[:-21, :-1, :]\n",
    "    x_test = windowed_data[-21:, :-1, :]\n",
    "    \n",
    "    if only_price == True:\n",
    "        y_train = windowed_data[:-21, -1, :]\n",
    "        y_test = windowed_data[-21:, -1, :]\n",
    "    else:\n",
    "        y_train = windowed_data[:-21, -1, :]\n",
    "        y_test = windowed_data[-21:, -1, :]\n",
    "    \n",
    "    return (x_train, y_train, x_test, y_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9038a",
   "metadata": {},
   "source": [
    "## Model Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b5941f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"LSTM object, generates lstm model with pytorch graph, variable geometry.\"\n",
    "    def __init__(self, inputs, hiddens, layers, outputs):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hiddens = hiddens\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(inputs, hiddens, layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hiddens, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initialize hidden\n",
    "        h0 = torch.zeros(self.layers, x.size(0), self.hiddens).requires_grad_()\n",
    "        # init cell\n",
    "        c0 = torch.zeros(self.layers, x.size(0), self.hiddens).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234fb09",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3e1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, loss_function, optimiser, epochs, window):\n",
    "    history = np.zeros(epochs)    \n",
    "    for epoch in range(epochs):\n",
    "        # forward pass\n",
    "        y_train_pred = model(x_train)\n",
    "\n",
    "        loss = loss_function(y_train_pred, y_train)\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch \", epoch, \"MSE: \", loss.item())\n",
    "        history[epoch] = loss.item()\n",
    "        \n",
    "        # epoch-wise gradient reset\n",
    "        optimiser.zero_grad()\n",
    "        # backpass\n",
    "        loss.backward()\n",
    "        # hyperparam update\n",
    "        optimiser.step()\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b09c2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(history, epochs, title):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=np.linspace(0, epochs), y=history,\n",
    "                         mode='lines',\n",
    "                         name='Loss'))\n",
    "    fig.update_layout(title='LSTM Training Performance',\n",
    "             xaxis_title='Epoch',\n",
    "             yaxis_title='Loss')\n",
    "    \n",
    "    fig.write_image(\"images/\"+title)\n",
    "    print(\"Figure saved to file at: images/\"+title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fa8ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_univariate_model(mixed_data, window=10, epochs=100):\n",
    "    xtr, ytr, xte, yte, scaler = prep_data(mixed_data, window, True) # univariate data prep\n",
    "    \n",
    "    x_train = torch.from_numpy(xtr).type(torch.Tensor)\n",
    "    x_test = torch.from_numpy(xte).type(torch.Tensor)\n",
    "    y_train = torch.from_numpy(ytr).type(torch.Tensor)\n",
    "    y_test = np.array(mixed_data['Close'], dtype='float')[-21:]\n",
    "    \n",
    "    model = LSTM(inputs=1, hiddens=32, layers=2, outputs=1)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    hist = train(model, x_train, y_train, loss_function, optimiser, epochs, window)\n",
    "    plot_training_loss(hist, epochs, \"univariate_model_training.png\") # training performance plot\n",
    "    \n",
    "    return (model, scaler, x_test, y_test, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323428d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multivariate_model(mixed_data, window=10, epochs=100):\n",
    "    xtr, ytr, xte, yte, scaler = prep_data(mixed_data, window, False) # multivariate data prep\n",
    "    \n",
    "    x_train = torch.from_numpy(xtr).type(torch.Tensor)\n",
    "    x_test = torch.from_numpy(xte).type(torch.Tensor)\n",
    "    y_train = torch.from_numpy(ytr).type(torch.Tensor)\n",
    "    y_test = torch.from_numpy(yte).type(torch.Tensor)\n",
    "    \n",
    "    model = LSTM(inputs=4, hiddens=32, layers=2, outputs=4)\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    hist = train(model, x_train, y_train, loss_function, optimiser, epochs, window)\n",
    "    plot_training_loss(hist, epochs, \"multivariate_model_training.png\") # training performance plot\n",
    "    \n",
    "    return (model, scaler, x_test, y_test, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb3930",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27a81481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation(df, df_result, title):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df_result.index, y=df_result['Reference'], name='Target',\n",
    "                             line=dict(width=2)))\n",
    "    fig.add_trace(go.Scatter(x=df_result.index, y=df_result['Predicted'], name='Prediction',\n",
    "                             line=dict(width=2, dash='dot')))\n",
    "    fig.update_layout(title='LSTM Inference Result',\n",
    "             xaxis_title='Date',\n",
    "             yaxis_title='Closing Price ($)')\n",
    "    \n",
    "    fig.write_image(\"images/\"+title)\n",
    "    print(\"Figure saved to file at: images/\"+title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de8b22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_focused_evaluation(df, df_result, title):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=df_result.index[-40:], y=df_result['Reference'][-40:], name='Target',\n",
    "                             line=dict(width=2)))\n",
    "    fig.add_trace(go.Scatter(x=df_result.index[-40:], y=df_result['Predicted'][-40:], name='Prediction',\n",
    "                             line=dict(width=2, dash='dot')))\n",
    "    fig.update_layout(title='LSTM Inference Result',\n",
    "             xaxis_title='Date',\n",
    "             yaxis_title='Closing Price ($)')\n",
    "    \n",
    "    fig.write_image(\"images/\"+title)\n",
    "    print(\"Figure saved to file at: images/\"+title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73392f0",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe5d2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_univariate(df, model, x_train, x_test, y_train, y_test, scaler):\n",
    "    y_pred = model(x_test)\n",
    "    y_pred = scaler.inverse_transform(y_pred.detach().numpy())\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "    \n",
    "    train_pred = model(x_train)\n",
    "    train_pred = scaler.inverse_transform(train_pred.detach().numpy())\n",
    "    train = scaler.inverse_transform(y_train.detach().numpy())\n",
    "    \n",
    "    rmse_test = math.sqrt(mean_squared_error(y_test[:, 0], y_pred[:, 0]))\n",
    "    rmse_train = math.sqrt(mean_squared_error(train[:, 0], train_pred[:, 0]))\n",
    "    \n",
    "    print('Train: %.2f RMSE' % (rmse_train))\n",
    "    print('Test: %.2f RMSE' % (rmse_test))\n",
    "    \n",
    "    result = pd.DataFrame(index=df.index[10:])\n",
    "    result['Reference'] = np.concatenate((train[:,0], y_test[:,0]))\n",
    "    result['Predicted'] = np.concatenate((train_pred[:,0], y_pred[:,0]))\n",
    "    \n",
    "    plot_evaluation(df, result, \"univariate_overall_evaluation.png\")\n",
    "    plot_focused_evaluation(df, result, \"univariate_forecast_evaluation.png\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "017b4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multivariate(df, model, x_train, x_test, y_train, y_test, scaler):\n",
    "    # select only price data point for eval metrics\n",
    "    y_pred = model(x_test)\n",
    "    y_pred = scaler.inverse_transform(y_pred.detach().numpy())\n",
    "    y_test = scaler.inverse_transform(y_test.detach().numpy())\n",
    "    \n",
    "    train_pred = model(x_train)\n",
    "    train_pred = scaler.inverse_transform(train_pred.detach().numpy())\n",
    "    train = scaler.inverse_transform(y_train.detach().numpy())\n",
    "    \n",
    "    rmse_test = math.sqrt(mean_squared_error(y_test[:, 0], y_pred[:, 0]))\n",
    "    rmse_train = math.sqrt(mean_squared_error(train[:, 0], train_pred[:, 0]))\n",
    "    \n",
    "    print('Train: %.2f RMSE' % (rmse_train))\n",
    "    print('Test: %.2f RMSE' % (rmse_test))\n",
    "    \n",
    "    result = pd.DataFrame(index=df.index[10:])\n",
    "    result['Reference'] = np.concatenate((train[:,0], y_test[:,0]))\n",
    "    result['Predicted'] = np.concatenate((train_pred[:,0], y_pred[:,0]))\n",
    "    \n",
    "    plot_evaluation(df, result, \"multivariate_overall_evaluation.png\")\n",
    "    plot_focused_evaluation(df, result, \"multivariate_forecast_evaluation.png\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd653285",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07f15f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Generate data ingress objects -- price and auxillary data sources\n",
    "    yahoo_finance = YahooFinance(False, True)\n",
    "    google_trends = GoogleTrends(True, True)\n",
    "    \n",
    "    # Load data from: API or local cold-storage (csv) and bring data into scope\n",
    "    yahoo_finance.load()\n",
    "    google_trends.load()\n",
    "    \n",
    "    # Write price and auxillary data from object scope to local hot-storage (fast and efficient read/write)\n",
    "    yahoo_finance.write()\n",
    "    google_trends.write()\n",
    "    \n",
    "    # Now, to demonstrate that the data is well and truly stored and not simply loaded in from scope\n",
    "    # we create two new data type objects, both without `load`, `write` permissions\n",
    "    yahoo_finance = 0 # forcefully 'kill' old data objects\n",
    "    google_trends = 0\n",
    "    \n",
    "    # New data type objects\n",
    "    yahoo = YahooFinance(False, False)\n",
    "    google = GoogleTrends(True, False)\n",
    "    \n",
    "    # Fetch the data from hot-storage (from Apache Arrow, feather file format)\n",
    "    yahoo.fetch()\n",
    "    google.fetch()\n",
    "    \n",
    "    # Create two datasets; one for the price-only model1, the other for the auxillary features model2\n",
    "    # Merge dataset, 'floodfill' applied to googtrends data\n",
    "    price_data = yahoo.data['Close']\n",
    "    mixed_data = merger(yahoo.data, google.data)\n",
    "    \n",
    "    # Data exploration\n",
    "    # You can find all locally saved figure in `images/`\n",
    "    \n",
    "    # Google trends 'floodfill'\n",
    "    plot_trend_filling(mixed_data, google.data)\n",
    "    \n",
    "    # Feature correlation heatmap\n",
    "    plot_correlation_heatmap(mixed_data)\n",
    "    \n",
    "    # Statistical analysis\n",
    "    static_analysis = StaticAnalysis(mixed_data)\n",
    "    static_analysis.anova_test() # mean hypothesis\n",
    "    static_analysis.friedman_test() # distribution hypothesis\n",
    "    \n",
    "    # Time series decompostion analysis\n",
    "    # breaks down a signal into 4 component: trends, seasonality, cycle, residual\n",
    "    automatic_tsd(mixed_data)\n",
    "    \n",
    "    # Model training:\n",
    "    # calls to data processing occurs within model training routine\n",
    "    window = 10 # size of look-back LSTM window\n",
    "    model_u, scaler_u, xte_u, yte_u, xtr_u, ytr_u = train_univariate_model(mixed_data, window)\n",
    "    model_m, scaler_m, xte_m, yte_m, xtr_m, ytr_m = train_multivariate_model(mixed_data, window)\n",
    "    \n",
    "    # Model evaluation:\n",
    "    evaluate_univariate(mixed_data, model_u, xtr_u, xte_u, ytr_u, yte_u, scaler_u)\n",
    "    evaluate_multivariate(mixed_data, model_m, xtr_m, xte_m, ytr_m, yte_m, scaler_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64aa0bc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data requested from Yahoo! Finance API (via yfinance)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Files loaded from: data/raw/\n",
      "File written at: data/store/market_data\n",
      "Data written at: data/store/google_trends\n",
      "Data fetched from: data/store/market_data\n",
      "Data fetched from: data/store/google_trends\n",
      "Figure saved to file at: images/google_trends_filling_effect.png\n",
      "Figure saved to file at: images/closing_price_feature_correlation.png\n",
      "stat=2245.219, p=0.000\n",
      "H1: Probably different distributions\n",
      "stat=1831.266, p=0.000\n",
      "H1: Probably different distributions\n",
      "Figure saved to file at: images/tsd_price.png\n",
      "Figure saved to file at: images/tsd_search.png\n",
      "Figure saved to file at: images/tsd_news.png\n",
      "Figure saved to file at: images/tsd_financial.png\n",
      "Epoch  0 MSE:  0.3304557800292969\n",
      "Epoch  10 MSE:  0.020251963287591934\n",
      "Epoch  20 MSE:  0.01966392807662487\n",
      "Epoch  30 MSE:  0.010228545404970646\n",
      "Epoch  40 MSE:  0.004828971344977617\n",
      "Epoch  50 MSE:  0.003193236654624343\n",
      "Epoch  60 MSE:  0.002464361721649766\n",
      "Epoch  70 MSE:  0.0019200900569558144\n",
      "Epoch  80 MSE:  0.0017769680125638843\n",
      "Epoch  90 MSE:  0.0017290430841967463\n",
      "Figure saved to file at: images/univariate_model_training.png\n",
      "Epoch  0 MSE:  0.3018440902233124\n",
      "Epoch  10 MSE:  0.05014127865433693\n",
      "Epoch  20 MSE:  0.034170038998126984\n",
      "Epoch  30 MSE:  0.02705385722219944\n",
      "Epoch  40 MSE:  0.022510942071676254\n",
      "Epoch  50 MSE:  0.01982649601995945\n",
      "Epoch  60 MSE:  0.01701713353395462\n",
      "Epoch  70 MSE:  0.014257250353693962\n",
      "Epoch  80 MSE:  0.011381139978766441\n",
      "Epoch  90 MSE:  0.008450017310678959\n",
      "Figure saved to file at: images/multivariate_model_training.png\n",
      "Train: 3.71 RMSE\n",
      "Test: 9.64 RMSE\n",
      "Figure saved to file at: images/univariate_overall_evaluation.png\n",
      "Figure saved to file at: images/univariate_forecast_evaluation.png\n",
      "Train: 4.17 RMSE\n",
      "Test: 9.89 RMSE\n",
      "Figure saved to file at: images/multivariate_overall_evaluation.png\n",
      "Figure saved to file at: images/multivariate_forecast_evaluation.png\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95927cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
